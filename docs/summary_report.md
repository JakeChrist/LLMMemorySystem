# ğŸ§  LLMemory Implementation Plan â€” Summary Report

## ğŸ”¹ Introduction

The **LLMemory system** is a local-first, reconstructive memory architecture for AI agents inspired by human cognition. It supports episodic, semantic, and procedural memory with emotion modeling and background dreaming for schema formation and long-term context reconstruction. It is pluggable, LLM-agnostic, and designed to help agents evolve their behavior over time.

---

## Dependencies

- Python 3.10+
- [sentence-transformers](https://www.sbert.net/) *(optional)* â€” used by `encoding/encoder.py` for generating real embeddings when installed.

---

## ğŸ“ Project Directory Structure

```
llmemory/
â”œâ”€â”€ main.py                    # Entry point
â”œâ”€â”€ config/                    # Configuration files
â”œâ”€â”€ core/                      # Core logic including memory types and agent
â”‚   â””â”€â”€ memory_types/          # Episodic, semantic, procedural memory
â”œâ”€â”€ encoding/                  # Text-to-vector encoding and tagging
â”œâ”€â”€ retrieval/                 # Cue construction and memory retrieval
â”œâ”€â”€ reconstruction/            # Reconstruct working memory context
â”œâ”€â”€ dreaming/                  # Background summarization and pruning
â”œâ”€â”€ storage/                   # Persistent storage and vector indexing
â”œâ”€â”€ llm/                       # Pluggable LLM backends (OpenAI, Claude, LMStudio, etc.)
â”œâ”€â”€ utils/                     # Logging and background task management
â”œâ”€â”€ cli/                       # Command-line interface
â”œâ”€â”€ tests/                     # Unit tests
â”œâ”€â”€ docs/                      # Documentation
â””â”€â”€ gui/                       # Optional PyQt GUI with conversation view
```

---

## âœ… Features Implemented So Far

- ğŸ§± Modular folder structure scaffolded and zipped for immediate development.
- ğŸ”Œ Pluggable LLM interface design for OpenAI, Claude, Gemini, LMStudio, etc.
- ğŸ§  Placeholder files for all core systems: encoding, retrieval, reconstruction, dreaming.
- ğŸªŸ **Optional PyQt5 GUI**:
  - Three-panel layout: left (user inputs), center (LLM responses), right (agent state).
  - Dummy placeholders for memory, mood, and dreaming.
- ğŸ“¦ Downloadable project ZIP with code templates and GUI included.

### ğŸš€ Launching the GUI

Create an ``Agent`` instance and pass it to ``run_gui``:

```python
from core.agent import Agent
from gui.qt_interface import run_gui

agent = Agent("local")
run_gui(agent)
```

---

## ğŸ”œ Next Steps

### ğŸ”§ Core Memory System
- [ ] Implement `MemoryEntry` data structures with metadata, embeddings, emotion tags.
- [ ] Connect `encoder.py` to real embedding models (e.g., `sentence-transformers`).
- [ ] Develop `retriever.py` to return top-K relevant memories using FAISS or Chroma.
- [ ] Implement `reconstructor.py` to merge memory fragments into coherent prompts.

### ğŸ§  Emotion & Mood
- [ ] Add `emotion_model.py` logic using sentiment classifier.
- [ ] Use mood to bias retrieval and context generation.

### ğŸŒ™ Dreaming Subsystem
- [ ] Add `dream_engine.py` to run in background or on schedule.
- [ ] Summarize related memories, generate schemas, prune redundancies.

### ğŸ¤– LLM Integration
- [ ] Implement each LLM backend wrapper (`openai_api.py`, `local_llm.py`, etc.).
- [ ] Use `llm_router.py` to dynamically select backend.
- [ ] Route agent output to selected LLM using reconstructed prompt.

### ğŸ–¥ï¸ GUI Polishing (Optional)
- [ ] Make PyQt GUI reactive to real memory data.
- [ ] Add mood/dreaming visualization (e.g., color-coded or icon-based).
- [ ] Enable editing or exploration of stored memories from the GUI.

### ğŸ§ª Testing & CLI
- [ ] Build `test_memory_workflow.py` to simulate full memory loop.
- [ ] Add CLI commands to query memory, trigger dreams, or reset state.

### CLI Usage

```
python -m cli.memory_cli list
python -m cli.memory_cli add "remember this" --model all-MiniLM-L6-v2
python -m cli.memory_cli query "cats" --top-k 3 --model all-MiniLM-L6-v2
python -m cli.memory_cli dream
python -m cli.memory_cli reset
python -m cli.memory_cli add-conversation chat.txt --agent bob
python -m cli.memory_cli add-biography bio.txt --agent bob
```

Use `--model` to select a sentence-transformers embedding model and `--top-k`
to control query result count.

### Launching with custom backends

You can run the main entry point with a specific LLM backend and database path:

```
python main.py repl --llm openai --db ./my.db
python main.py gui --llm openai --db ./my.db
python main.py cli --db ./my.db list
```

## License

This documentation is licensed under the [MIT License](../LICENSE). Copyright (c) 2024 Jacob Christ.
